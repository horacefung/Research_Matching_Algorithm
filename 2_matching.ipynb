{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firm Matching Code (Second iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "\n",
    "1. Read the cleaned + tokenized patent + mos firm names\n",
    "2. Read token frequencies for patent names\n",
    "3. Perform matching algo:\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "1. Loop through each token in firm names and match against the tokens of patent names\n",
    "2. Eliminate perfect matches (they have 0 distance and contributes 0 to weighted distance measure)\n",
    "3. For the remaining tokens, select the matches with minimum distance\n",
    "4. Use these matches and calculate a frequency-weighted average distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading in the files\n",
    "path_to_patent_token = \"/Users/horacefung/Desktop/1_Uni/Research/Code/code/processed_files/patent_token.pkl\"\n",
    "df_patent_token = pd.read_pickle(path_to_patent_token)\n",
    "\n",
    "path_to_mos_token = \"/Users/horacefung/Desktop/1_Uni/Research/Code/code/processed_files/mos_tokens.pkl\"\n",
    "df_mos_token = pd.read_pickle(path_to_mos_token)\n",
    "\n",
    "path_to_freq_patent = \"/Users/horacefung/Desktop/1_Uni/Research/Code/code/processed_files/patent_freq.pkl\"\n",
    "df_freq_patent = pd.read_pickle(path_to_freq_patent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>2093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>william</th>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alva</th>\n",
       "      <td>552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jerome</th>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         frequency\n",
       "company       2093\n",
       "john           668\n",
       "william        667\n",
       "alva           552\n",
       "jerome         494"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_freq_patent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we need to sample\n",
    "#df_mos_token = df_mos_token.sample(n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Removing perfect matches\n",
    "\n",
    "Create the dataframe common to store exact matches, then remove the common rows in both of the input dataframes by matching with the unique ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of tokens ['token1', 'token2'] into a standardized string format 'token1-token2'\n",
    "df_mos_token['token_string'] = df_mos_token['firm_names'].apply(lambda x: \"-\".join(x))\n",
    "df_patent_token['token_string'] = df_patent_token['firm_names'].apply(lambda x: \"-\".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can drop duplicates on the patent_data side. Just have to keep the entire df_mos.\n",
    "df_patent_token = df_patent_token.drop_duplicates(subset=['token_string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on the token string and preserve only the Id columns\n",
    "common = df_patent_token.merge(df_mos_token,on=['token_string','token_string'])\n",
    "common = common[['id_x', 'id_y']]\n",
    "common = common.rename(columns = {'id_x': 'assignee_patent_id','id_y': 'mos_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output\n",
    "common.to_csv('./processed_files/common.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the pefect matches from the two datasets we need to match\n",
    "df_mos_token = df_mos_token[~df_mos_token['id'].isin(common['mos_id'])].dropna()\n",
    "df_patent_token = df_patent_token[~df_patent_token['id'].isin(common['assignee_patent_id'])].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Matching Function\n",
    "\n",
    "Each row of the output follows this format:\n",
    "\n",
    "**['mos_id', top 5 patent_id matches, distances of the top 5 matches]**\n",
    "\n",
    "The matching function performs this operation for each run:\n",
    "\n",
    "1. It takes a token list from df_mos_token\n",
    "2. It scans across *all* token lists from df_patent_token\n",
    "3. For each one, it checks the difference between the number of tokens is smaller or equal to min_len e.g. 2\n",
    "4. Then it removes any tokens that match perfectly from the list and attempts to retrieve the token frequencies.\n",
    "5. The remaining tokens do not perfectly match. The function calculates the levenshtein distance for all the a  \n",
    "   possible combinations of these tokens using `editdistance`.\n",
    "6. Using `lap`, the function construct a distance matrix between the mos and patent tokens. It will optimize\n",
    "   the matrix as an assignment problem by creating a binary variable matrix that represents which token is \n",
    "   assigned to which, constraint by the fact that each mos_token/patent_token can only be assigned once. The\n",
    "   sumproduct between this assignment matrix and the distance matrix will be the total distance, and `lap` will\n",
    "   determine an assignment that minimizes it.\n",
    "7. Given the assignment, the function retreives the frequencies of the tokens and stores it.\n",
    "8. The function computes the frequency weighted distance measure.\n",
    "9. The function compares this distance with a list of previous match distances, and replaces the largest distance\n",
    "   in the list.\n",
    "10. It will always maintain a top_n list, e.g. top 5 matches.\n",
    "11. It will yield the top_n matches for each mos_token input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import editdistance\n",
    "import lap\n",
    "import numpy as np\n",
    "\n",
    "def matching_algorithm(df_mos_token, df_patent_token, df_freq_patent, top_n, min_len):\n",
    "    \n",
    "    for mos_list in df_mos_token.itertuples():\n",
    "        dist_top_matches = []\n",
    "        id_top_matches = []\n",
    "        ## Scan through all patent names for each mos name\n",
    "        for patent_list in df_patent_token.itertuples():\n",
    "            ## length of names/number of tokens should be close\n",
    "            ## Skip those with a large difference\n",
    "            len1 = patent_list[3]\n",
    "            len2 = mos_list[3]\n",
    "            diff = abs(len1 - len2)\n",
    "            if diff >= min_len:\n",
    "                continue\n",
    "\n",
    "            ## Sum up the 1/freq weights on the denominator separately\n",
    "            denominator_freq = 0 \n",
    "\n",
    "            ## Remove perfectly matched strings from both lists aka. dist==0\n",
    "            intersection = Counter(mos_list[2]) & Counter(patent_list[2])\n",
    "            mos_unique_list = Counter(mos_list[2]) - intersection\n",
    "            mos_unique_list = tuple(mos_unique_list) #structure creation at small scale, tuple should be faster\n",
    "            patent_unique_list = Counter(patent_list[2]) - intersection\n",
    "            patent_unique_list = tuple(patent_unique_list)\n",
    "\n",
    "            #-----\n",
    "            # Perfect Token Matches Path\n",
    "            # \n",
    "            # Deal with perfectly matched tokens, \n",
    "            # where weighted numerator = 1/freq *(0+1)\n",
    "            #-----\n",
    "\n",
    "            # Accounting for perfect token matches\n",
    "            perfect_numerator = 0\n",
    "            for perfect_match in intersection:\n",
    "                #dist ==0, which means token numerator is (1/f)*(0+1) = 1/f\n",
    "                try:\n",
    "                    #try if token is in assignee patent\n",
    "                    freq = df_freq_patent.loc[perfect_match][0]\n",
    "                    perfect_numerator += 1/(freq)\n",
    "                    denominator_freq += 1/(freq)\n",
    "                except:\n",
    "                    #if it's not in assignee patent, set as 1\n",
    "                    perfect_numerator += 1\n",
    "                    denominator_freq += 1\n",
    "\n",
    "            #-----\n",
    "            # Imperfect Token Matches Path\n",
    "            #\n",
    "            # Any remaining tokens need to be matched such \n",
    "            # that the total distance between these tokens are minimized.\n",
    "            #-----\n",
    "\n",
    "            imperfect_dist = []\n",
    "            imperfect_freq = []\n",
    "\n",
    "            if len(mos_unique_list) != 0 and len(patent_unique_list) != 0: # test for leftover tokens\n",
    "                ## Get a list of distance for each token from patent, then stack into a matrix\n",
    "                dist_matrix = []\n",
    "                for token1 in patent_unique_list:\n",
    "                    dist_list = []\n",
    "                    for token2 in mos_unique_list:\n",
    "                        dist = editdistance.eval(token1, token2)\n",
    "                        dist_list.append(dist)\n",
    "                    dist_matrix.append(dist_list)\n",
    "\n",
    "                ## Linear programming, assignment with lapjv, minimizing total word distance \n",
    "                x, y = lap.lapjv(np.array(dist_matrix), extend_cost=True, return_cost=False)\n",
    "\n",
    "                for row, column in zip(dist_matrix, x):\n",
    "                    #lapjv sets columns not selected in an MxN matrix (M != N) as -1, we ignore that\n",
    "                    if column >= 0:\n",
    "                        ## Append the optimal distances into imperfect_dist\n",
    "                        imperfect_dist.append(row[column])\n",
    "                        ## Select the token \n",
    "                        mos_token_x = mos_unique_list[column]\n",
    "                        try:\n",
    "                            # Try to find frequency in assignee patent\n",
    "                            freq = df_freq_patent.loc[mos_token_x][0] # get frequency of selected mos tokens\n",
    "                            imperfect_freq.append((1/(freq))) \n",
    "                            denominator_freq += (1/(freq)) # keep adding to denominator                \n",
    "                        except:\n",
    "                            # If the token is not in the assignee_patent, set it to 1\n",
    "                            imperfect_freq.append(1)\n",
    "                            denominator_freq += 1\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "            #------\n",
    "            # Merging perfect and imperfect matches to \n",
    "            # calculate our weighted distance measure\n",
    "            #------\n",
    "\n",
    "            ## Calculate denominator\n",
    "            denominator = sum(imperfect_dist) * denominator_freq\n",
    "\n",
    "            ## Calculate numerator\n",
    "            imperfect_dist = [j+1 for j in imperfect_dist]\n",
    "            imperfect_numerator = np.dot(np.array(imperfect_dist), np.array(imperfect_freq))\n",
    "            numerator = imperfect_numerator + perfect_numerator\n",
    "\n",
    "            ## Accounting for full matches but not perfect [Westinghouse, Mfg] to [Westinghouse], denominator = 0\n",
    "            ## No 0+1 clause on denominator distances\n",
    "            if denominator == 0:\n",
    "                distance_measure = 0\n",
    "            else:\n",
    "                distance_measure = (numerator/(denominator))\n",
    "\n",
    "            ## Create/append into top 5 matches\n",
    "            if len(dist_top_matches) == top_n:\n",
    "                ## Deciding which match to replace if there is already 5 matches \n",
    "                max_dist = max(dist_top_matches) # check if new distance is smaller than at least 1 \n",
    "                if distance_measure < max_dist: # if so, we replace it with new distance/firm_names\n",
    "                    list_index = dist_top_matches.index(max_dist)\n",
    "                    dist_top_matches[list_index] = distance_measure # replace the dist value with max index num\n",
    "                    id_top_matches[list_index] = patent_list[1] # replace firm name with same index\n",
    "                    continue\n",
    "            elif len(dist_top_matches) < top_n:\n",
    "                # this else only happens if fewer than 5 entries\n",
    "                dist_top_matches.append(distance_measure)\n",
    "                id_top_matches.append(patent_list[1]) # firm id from assignee.pkl file\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "        output_list = [mos_list[1]] + id_top_matches + dist_top_matches #pure id output\n",
    "        yield(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = matching_algorithm(df_mos_token.head(10), df_patent_token, df_freq_patent, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a connection to the file\n",
    "start = time.time()\n",
    "with open('./processed_files/test.csv', \"w\") as file:\n",
    "    writes = csv.writer(file, delimiter=',')\n",
    "    writes.writerows(test)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3909, 4010929, 1933154, 2605658, 4777188, 2855252, 0.28225806451612906, 0.28225806451612906, 0.28225806451612906, 0.28225806451612906, 0.2833333333333333]\n",
      "[4818, 5424739, 2846683, 916908, 2362100, 2642302, 0.5476190476190477, 0.5476190476190477, 0.5476190476190477, 0.55, 0.5476190476190477]\n",
      "[3342, 1007351, 2164634, 4815939, 4452903, 1438064, 0.37037037037037035, 0.36904761904761907, 0.37037037037037035, 0.37037037037037035, 0.3717948717948718]\n",
      "[130, 3124846, 4748860, 5577804, 2576401, 5424713, 0, 1.0625, 1.0833333333333333, 1.0666666666666667, 0]\n",
      "[8222, 3342891, 2981413, 1743781, 4547298, 4527067, 0.242298736889823, 0.21707147291780332, 0.2569482288828338, 0.2576468543621828, 0.242319342276071]\n",
      "[1813, 2846683, 3525786, 2242274, 1980451, 4851699, 0.2907268170426065, 0, 0.3333333333333333, 0, 0.3333333333333333]\n",
      "[7874, 3044330, 2743171, 2846683, 1264563, 1300022, 0.4583333333333333, 0.4583333333333333, 0.45000000000000007, 0.4642857142857143, 0.46875]\n",
      "[1533, 4815939, 4452903, 1438064, 2164634, 3272132, 0.37037037037037035, 0.3717948717948718, 0.37037037037037035, 0.3717948717948718, 0.3717948717948718]\n",
      "[6461, 2173921, 3468177, 2282235, 3466081, 5640340, 0.2716049382716049, 0.2576489533011272, 0.2700617283950617, 0.2623456790123456, 0.27053140096618356]\n",
      "[378, 4425640, 3165181, 4532949, 3127574, 2530537, 0.30024844720496896, 0.2898332788492971, 0.3097900029577048, 0.3097900029577048, 0.3097900029577048]\n",
      "17.059134006500244\n"
     ]
    }
   ],
   "source": [
    "#Sample output\n",
    "start = time.time()\n",
    "for i in test:\n",
    "    print(i)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
